\graphicspath{{./images/chap7/}}
% Classification with CNN
% Methodology
% * Design and Architecture
% * Population of interest and sampling subject used in the study
% * Instrument and what it measures (metrics)
% * qualifications of informants if used in the study
% * Validation
% * Data gathering procedure (experiments)
\chapter{Convolutional Neural Network} % (fold)
\label{cha:convolutional_neural_network}

We first describe our new architecture in System Overview
(Section~\ref{sec:system_overview}). Then in Experiments
(Section~\ref{sec:experiments}), we compare the performance of the current
identification method within the existing pattern retrieval engine with that of
the new architecture.

\section{System Overview} % (fold)
\label{sec:system_overview}

In this section, we begin by presenting the architecture that outputs a
yes-or-no answer to the question whether, given two images A and B, the animal
in image A is the same individual as the animal in image B.

The proposed architecture comprises two major modules shown in
Figure~\ref{fig:cnn_overview}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{system/overview}
  \caption{System Overview}
  \label{fig:cnn_overview} %chktex 24
\end{figure}

\begin{description}
    \item[Feature Extraction] We use a pre-trained CNN with the last
        fully-connected layer (the output layer) removed to extract fixed-size feature
  vectors from the input images before feeding them into the \emph{Match
  Recognition} module.
  \item[Match Recognition]
\end{description}

\subsection{Feature Extraction} 

For a pattern retrieving engine such as Sloop, the system continually
accumulates more data over time. The availability of abundant data enables
learning based methods to outperform engineered features because they can
discover and optimize features for the specific task at hand.

Instead of hand-picking the features to extract similarity matrices from the
data, a convolutional neural network (CNN) will be used as a feature extractor,
similar to the solution proposed in~\cite{chopra05} for face verification.

We use a pre-trained CNN, AlexNet by Krizhevsky et al.~\cite{kriz12} trained on
ImageNet~\cite{imagenet}, which contains 1.2 million images with 1000
categories. To convert the CNN into a feature extractor, we strip out the last
fully-connected layer, which in this case, outputs 1000 class scores for the
ImageNet classification task. According to the AlexNet architecture, this would
output a sparse 4096 dimensional vector for every image. This technique is
called transfer learning~\cite{transfer}.

We map all the available images as well as the input image to points in a low
dimensional space (4096-D compared to $227 \times 227$-D) using the
aforementioned CNN architecture. 

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
      \centering
      \includegraphics[width=7cm]{preprocess/paramconv1}
      \caption{The first layer filters, \texttt{conv1}}
  \end{subfigure}
  ~\\
  \begin{subfigure}[t]{0.45\textwidth}
      \centering
      \includegraphics[width=7cm]{preprocess/blob20conv1}
      \caption{The first layer output, \texttt{conv1}}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.45\textwidth}
      \centering
      \includegraphics[width=7cm]{preprocess/blob20pool5}
      \caption{The fifth layer after pooling, \texttt{pool5}}
  \end{subfigure}
  ~ \\
  \begin{subfigure}[t]{0.45\textwidth}
      \centering
      \includegraphics[width=7cm]{preprocess/lastconv1}
      \caption{The first layer output, \texttt{conv1}}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.45\textwidth}
      \centering
      \includegraphics[width=7cm]{preprocess/lastpool5}
      \caption{The fifth layer after pooling, \texttt{pool5}}
  \end{subfigure}
  \captionsetup{justification=centering}
  \caption{Visualizing Convolutional Network layers}
\end{figure}


\textbf{Pre-trained Convolutional Neural Network}

The main advantage of using a pre-trained CNN for our application is that it is
robust to geometric distortion. This is a desirable property since the position
of the individuals in our dataset is not aligned. This enables us to
accurately recognize the animal regardless of its position in the image. 

The translation invariant property emerges as a result of the contiguity in our
pooling regions and the fact that we only pool features generated from the same
hidden units~\cite{ufldl}. Even though this property is desirable to reduce the
translation noise, this prevents learning some features that are described by
their relative positions. For example, individual $A$ with a star-shaped spot
right above its eye may be confused with another individual $B$ who also has
star-shaped spot underneath its eye. 

The solution to this problem is to experiment with different patterns of pooling
regions and relax the parameter sharing scheme. Since we are using an existing
pre-trained architecture, it is hard to tailor it to fit our data. This improvement
can be included in future work (see Chapter~\ref{cha:future_work}).

\textbf{Caffe}

The CNN is implemented in Python using Caffe~\cite{caffe}, a deep learning
framework developed by the Berkeley Vision and Learning Center (BVLC).

\subsection{Match Recognition}

Once we have the 4096-D vectors for all images generated with our CNN feature
extractor, we transfer them into a second target match recognizer and train it
on a target dataset and task. The match recognizer is a linear classifier that,
given a pair of image vectors, decides whether the two images contain the same
individual.

\textbf{Input Processing}

Upon receiving the input from the feature extractor, we process the input using
one of two methods before passing it into the recognizer, which outputs a
binary label determining whether the pair (A, B) is a match.

\begin{enumerate}
\item Concatenation

Given two input image vectors $\vec{A}$ and $\vec{B}$ (flattened), we
concatenate them horizontally so that the output $\vec{E}$ becomes $\vec{E} =
[ \vec{A}\, \vec{B} ]$. To make the order deterministic, we always start with
the vector with the smaller sum. For equal sums, we compare each pair of
corresponding entries in both vectors until we find an entry with a smaller
element.

We can visualize this as feeding a tuple of image vectors $(\vec{A}, \vec{B})$
to a linear classifier. This is equivalent to giving the classifier all the
information we have and expecting it to figure out the optimal parameters.
However, in this case there is no separator indicating that two image vectors
are actually separated.

\item Computing the absolute difference

We would like to find a similarity metric that represents how much two images
differ from each other. In order to achieve that we came up with following
metric:
$$\vec{E} = |\vec{A} - \vec{B}|$$
Each entry $e_i \in \vec{E}$ is
small if $\vec{A}$ and $\vec{B}$ belong to the same category, and large if
different.  Not only does this encapsulate the desired numerical behavior, but it
also eliminates the absence of the separator problem.
  
\end{enumerate}

\textbf{Recognizer}

Images can be uploaded to Sloop one by one or in a batch. One major difference
is that batch processing keeps the system weights constant while computing
the error associated with each sample in the input, whereas the on-line version
constantly updates its weights. For a real-time system like Sloop, we would
prefer the on-line version of the classification algorithm given the same
classification performance.

We have implemented the following algorithms as our match recognizer:
\begin{itemize}
  \item Linear support vector machine (SVM) with L2 regularization (squared
    Euclidean norm)
  \item SVM with radial basis function kernel (SVM-RBF), L2 regularization
  \item Perceptron (P)
  \item Passive-Aggressive with hinge loss (PA-I)
  \item Passive-Aggressive with squared hinge loss (PA-II)
\end{itemize}

We compare the performance of each classifier in Experiments
(Section~\ref{sec:experiments}). The classifier with the best performance will
be selected for our final design.
% section system_overview (end)

\section{Experiments} % (fold)
\label{sec:experiments}

In this section, we first introduce the datasets used in the experiments, then
present a detailed evaluation of each variant of our architecture and the
comparison between the presented solutions.

\subsection{Dataset}

The training and testing data is available in the existing Sloop system. We
base our experiments on both species of skinks: \emph{Grand} and \emph{Otago}
mentioned in Chapter~\ref{cha:datasets}.

For our experiments, all images are reduced to the size of
$227 \times 227$ pixels so that it can be directly fed into the CNN
architecture to speed up feature extraction. Regardless of our preprocessing,
the CNN will resize the images into $227 \times 227$ to fit its input
dimension. However, resizing is not necessary because practically our system
should maintain its identification capability without regard to the variations
in sizes, lighting, or background.

\textbf{Partitioning}

We divide the data from both databases into four datasets by animal species:
Gr$^{L}$-I, Gr$^{L}$-II, Ot$^{L}$-I, Ot$^{L}$-II where Gr is for Grand, Ot is
for Otago, and $L$ is for the left view. Notice that the right view is not be used for the
experiments because the results should be similar to that of left view by
symmetry. In addition, due to our memory limit during the processing, each
dataset contains only 300 images of individuals whose image per individuals are
greater than 11.

For the purpose of generating a test set that resembles the empirical input and
another test set with images that are not seen during training, we build
our datasets using two different techniques.
\begin{enumerate}
  \item Gr$^{L}$-I, Ot$^{L}$-I represents the empirical input where the data in
  the test set can also be seen in the training set.
  \item Gr$^{L}$-II, Ot$^{L}$-II represents the test set with images that are
  not seen during training. The set of individuals is split into three
  disjoint sets for training, validating, and testing. Each image in a set can
  only be paired with the images within the same set.
\end{enumerate}

For each dataset, we partition the set of \emph{generated pairs} into three
sets: the train set (60\%), the validation set (20\%), and the test set (20\%).
We prevent the overfitting problem by monitoring the model's performance on a
validation set, acting as a representation of future test examples. If the
model's performance ceases to improve sufficiently on the validation set then
we test the model on the actual test set.


\begin{table}[t]
\captionsetup{justification=centering}
  \caption{Details of the train, validation, and test set for the four datasets}
\centering
  \begin{tabular}{llrcc}
    \toprule
    \multicolumn{2}{l}{Name (NTotal)}      & NPairs & \% Match & \% Non-match \\
    \midrule
    \multirow{3}{*}{Gr$^{L}$-I (10878)}  & Train & 6526 & 0.07 & 0.93 \\
                                 & Val   & 2175 & 0.07 & 0.93 \\
                          & Test  & 2177 & 0.07 & 0.93 \\
\midrule
\multirow{3}{*}{Ot$^{L}$-I (10878)}  & Train & 6526 & 0.06 & 0.94 \\
                                 & Val   & 2175 & 0.06 & 0.94 \\
                                 & Test  & 2177 & 0.06 & 0.94 \\
\midrule
  \multirow{3}{*}{Gr$^{L}$-II} & Train & 2926 & 0.13 & 0.87\\
                                 & Val   & 351 & 0.38 & 0.62\\
                                 & Test  & 1035 & 0.25 & 0.75\\
    \midrule
    \multirow{3}{*}{Ot$^{L}$-II} & Train & 3240 & 0.10 & 0.90\\
                                 & Val   & 561 & 0.31 & 0.69\\
                                 & Test  & 561& 0.26 & 0.73\\
    \bottomrule
  \end{tabular}
  \label{tab:results_tvt}
\end{table}
% section experiments (end)

% chapter convolutional_neural_network (end)
